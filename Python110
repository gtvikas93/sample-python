from flask import Flask, request, jsonify
from flask_restful import Api, Resource
import pandas as pd
from transformers import DistilBertTokenizer, DistilBertModel
import torch
from torch.nn.functional import cosine_similarity
import numpy as np

# Load DistilBERT model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')


# Function to encode text to embeddings using DistilBERT
def encode_text(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)


# Preprocess and load data from Excel
def load_data_from_excel(file_path):
    df = pd.read_excel(file_path, engine='openpyxl')
    # Combine all text columns into a single text (optional, based on your data)
    df['combined_text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
    return df


# Global variable to store embeddings (for demo purposes, better to handle with databases or caching in production)
data_df = load_data_from_excel('data.xlsx')
data_embeddings = encode_text(data_df['combined_text'].tolist())

# Flask app and API setup
app = Flask(__name__)
api = Api(app)


# Semantic search API Resource
class SemanticSearch(Resource):
    def post(self):
        json_data = request.get_json(force=True)
        query = json_data['query']
        top_k = json_data.get('top_k', 5)  # Return top_k results based on similarity

        query_embedding = encode_text([query])
        scores = cosine_similarity(query_embedding, data_embeddings).squeeze()

        # Create a list of (score, index) tuples and sort them by score in descending order
        if scores.ndim > 0:  # Multiple results
            results = [(score.item(), idx) for idx, score in enumerate(scores)]
        else:  # Single result
            results = [(scores.item(), 0)]

        # Sort results by score in descending order and select top_k results
        results.sort(reverse=True, key=lambda x: x[0])
        top_results = results[:top_k]

        # Prepare and return the sorted top_k results
        sorted_top_results = [{'score': score, 'data': data_df.iloc[idx].to_dict()} for score, idx in top_results]
        return jsonify(sorted_top_results)


api.add_resource(SemanticSearch, '/search')

if __name__ == '__main__':
    app.run(port=5000, debug=True)
